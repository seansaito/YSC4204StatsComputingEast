# Code for YSC4204 Homework 3
# Authors
# Sean Saito
# John Reid
# Hrishi Olickel
# Han Chong

#imports
require(ggplot2)

# Automatically sets working directory to source file location
this.dir <- dirname(parent.frame(2)$ofile)
setwd(this.dir)

# Problem 1

# (a)
# Option 1
# https://stat.ethz.ch/mailman/listinfo/r-devel
# A "list is intended for questions and discussion about code development in R. 
# Questions likely to prompt discussion unintelligible to non-programmers or topics 
# that are too technical for R-help???s audience should go to R-devel"
# 
# Option 2
# https://stat.ethz.ch/mailman/listinfo/r-package-devel
# This is the mailing list to all package developers of R
# To get " help about package development in R"

# (b)
# Draft of the question
# In src/nmath/sexp.c, the Ahrens-Dieter algorithm is being used to generate random numbers
# from an exponential distribution.
# 
# I am curious as to why this algorithm is favored over the inverse transform method, which is the
# straightforward method of producing random numbers from a particular probability distribution function, used
# as the default for normal random numbers.
# Since the cumulative distribution function of the exponential function is F_x(X) = 1 - e^{-lambda x},
# one would use its inverse, F_x^-1(u) = -(1/lambda)log(1-u), to produce samples. In R, this is as easy as
# -log(runif(n)) / lambda.
# 
# I have tried to hypothesize some benefits of the Ahrens-Dieter algorithm myself. However there does not seem to be 
# any asymptotic time benefits (the Ahrens-Dieter algorithm involves a single for-loop), nor any obvious improvements in precision.

# Problem 2
# Assessing a proposed alternative to the Box-Muller method

library(MVN)
library(mvtnorm)
set.seed(0)

# Helper functions
z1_gen <- function(u, v) {
  return(sqrt(-2 * log(u)) * cos(2 * pi * v))
}

z2_gen <- function(u, v) {
  return(sqrt(-2 * log(v)) * sin(2 * pi * u))
}

# Initialization
n = 2000
u <- runif(n)
v <- runif(n)

# Standard Bivariate Normal
biv <- rmvnorm(n,c(0,0))

# Get z1 and z2
z1 <- z1_gen(u, v)
z2 <- z2_gen(u, v)

# Plot standard bivariate normal
plot(biv[,1],biv[,2],main="Standard Bivariate Normal",xlab="X1",ylab="X2")

# Plot z1 against z2
plot(z1, z2, main="Z1 vs. Z2", xlab="Z1", ylab="Z2")

pair <- matrix(c(c(z1), c(z2)), ncol=2)
pair_df <- data.frame(pair)

# Assess the pair using MVN
# Useful documentation here: https://cran.r-project.org/web/packages/MVN/vignettes/MVN.pdf
mardiaResult <- mardiaTest(pair_df)
print(mardiaResult)
# Skew is too big, and thus is not multivariate normal
hzResult <- hzTest(pair_df)
print(hzResult)
# HZ statistic is too high - not multivariate normal
roystonResult <- roystonTest(pair_df)
print(roystonResult)
# Here, the Royston Test thinks the data is multivariate normal

# In the end, the majority of tests label the distribution as not multivariate normal.
# Even for the Royston Test, which determined the data as multivariate normal, the p-value is rather
# high at 0.2379013.
# The Q-Q plot, which plots the theoretical and observed quantiles of the different distributions, 
# also indicates that the points deviate from the line y=x which indicates multivariate normality.

# Let's try correcting Rizzo's function
z3_gen <- function(u, v) {
  return(sqrt(-2 * log(u)) * sin(2 * pi * v))
}
# get z3
z3 <- z3_gen(u, v)
# Plot z1 against z3
plot(z1, z3, main="Z1 vs. Z3", xlab="Z1", ylab="Z3") 

# By swapping the position of U and V in Rizza's function, the resulting distribution is much more similar in appearance as compared to our standard bivariate normal plot. Even without running hypothesis tests for the points generated by the corrected function, we are quite confident that the Rizzo's function is indeed a typo.

# Problem 3

# Part A - Box-Muller normal random numbers

RNGkind("default", "Box-Muller")
set.seed(536)
unif_rands <- runif(4)

set.seed(536)
rnorms <- rnorm(4)

# According to the C file snorm.c, the following two
# quantities are calculated, since we need to create 
# two independent random numbers each time we draw
theta = 2 * pi * unif_rands[1]
R = sqrt(-2 * log(unif_rands[2]))

# The normal random numbers are then generated as follows
norm1 = R * cos(theta)
norm2 = R * sin(theta)

# These numbers are exactly equal to eachother, according to R
cat(rnorms[1], ":", norm1, "  ", rnorms[1] == norm1,"\n")
cat(rnorms[2], ":", norm2, "  ", rnorms[2] == norm2,"\n")

# Part B - default inversion method

RNGkind("default", "default")
set.seed(536)
unif_rands <- runif(4)

set.seed(536)
rnorms <- rnorm(4)

# The code for the default inversion method is found in 
# snorm.c and qnorm.c According to snorm.c, we 
# increase precision by multiplying a uniform random number
# by 2^27, then adding another uniform number to it, and finally
# dividing by 2^27 again.

u1 = (unif_rands[1] * (2^27) + unif_rands[2])/2^27
u2 = (unif_rands[3] * (2^27) + unif_rands[4])/2^27

# We can clearly see that u1 and u2 have been changed - only
# the first six digits of u1 remain the same for example. 
print(format(c(u1,unif_rands[1]), digits = 20))
print(format(c(u2,unif_rands[3]), digits = 20))

# We then call qnorm5 in the file qnorm.c 
# with mu=0,sigma=1,lower_tail=1 and log_p=0

# qnorm defines q = p - 0.5
q1 <- u1 - 0.5
q2 <- u2 - 0.5

# We now approximate the integral with a rational 
# function, with constants supplied in the source code
# (calculated using the Remez function)
# Since abs(q1) < 0.425 and abs(q2) < 0.425 we do the following

r1 <- .180625 - q1 * q1
r2 <- .180625 - q2 * q2

rational_approx <- function(r, q){
  return(
  q * (((((((r * 2509.0809287301226727 +
               33430.575583588128105) * r + 67265.770927008700853) * r +
             45921.953931549871457) * r + 13731.693765509461125) * r +
           1971.5909503065514427) * r + 133.14166789178437745) * r +
         3.387132872796366608) / (((((((r * 5226.495278852854561 +
           28729.085735721942674) * r + 39307.89580009271061) * r +
         21213.794301586595867) * r + 5394.1960214247511077) * r +
       687.1870074920579083) * r + 42.313330701600911252) * r + 1.)
  )
}
final1 = rational_approx(r1, q1)
final2 = rational_approx(r2, q2)

print(format(c(final1,rnorms[1]), digits = 20))
print(format(c(final2,rnorms[2]), digits = 20))

# This looks accurate up to about nine decimal places, but
# there is some approximation error in the rational function


# Problem 5

# (a) Inverse Transform method 
#   Can we work through a few more examples of the inverse transform, perhaps
#   for slightly more tricky integrals?
#
# (b) Comparison of different C interfaces (.Call, .C, .Primitive, .External)
#
#   What are the advantages and disadvantages of each of these various methods?
#   Can we work through a more complex case of outsourcing computation to C?



